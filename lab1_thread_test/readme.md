### 1. 网页的选择与抓取：
选择“jwc.hit.edu.cn”教务处网站作为我们爬虫的起点网站，并限制爬虫程序只爬取此网站域名下的网页。首先我们设置Http请求报文的部分字段，将Cookie字段，User-Agent字段，Connection字段加入。接着从起始网址出发，初步筛选（将非jwc.hit域下的url筛去，将不允许爬取的url筛去）并爬取到1100个教务处网站下的网页url，并将其存入线程安全的Queue中，方便后续爬取网页内容和下载网页附件。接着我们需要根据网页内容判断该网页是否合法，即：该网页是否有标题\<title\>，是否有正文\<div class = “wp_articlecontent”\>，并检查是否有附件，附件是否能够正常下载。

### 2. 附件的下载：
首先我们需要识别附件，可以通过F12查看教务处网站附件的基本格式，均为jwc.hit.edu.cn/_upload/…/xxx.doc(xlxs, doc, txt, etc.)。所以我们只需要找到结尾是xlxs, doc, docx, txt等且包含‘_upload’的url即可。利用BeautifulSoup抓取网页上所有的<a>标签的href属性值（即url），接着用正则表达式筛选附件url即可。
	
### 3. 多线程抓取页面内容和下载：
利用threading库中的Thread, Lock类实现了多线程抓取网页和下载附件。继承Thread类并重写run函数实现Mythread类。run()函数中利用死循环（while True）实现了不同线程不断抓取不同url的过程，需要注意的是，在访问全局变量或记录全局变量时需要使用Lock.acquire()函数获取锁并阻塞进程，防止全局变量访问值错误。在访问完全局变量后要及时释放锁Lock.release()。合理利用锁我们能够为每个下载下来的附件创建对应json文件行数的目录，方便我们检查爬取的文件是否正确，json文件写入是否正确。页面内容的抓取根据F12查看的网页HTML源码找到对应标签即可。需要注意的是对于某些列表网页或网址导航主页，其正文部分的属性为<span class = “news_list”>，利用bs4抓取对应属性下的text即可。多线程使程序抓取网页速度明显变快。
	
### 4. 礼貌规则：
利用urllib的robotparser模块访问每个网站的../robot.txt目录并根据爬虫协议解读（本人爬虫名为HIT_IR_CRAW_ROBOT）本爬虫能够爬取哪些网页。同时设置爬取时间间隔，craw_tlimit，在重写的run函数中，每爬取一次网页则sleep一段时间后再次爬取。
	
### 5. 分词和去停用词处理：
利用本校的LTP模块实现了对标题和正文的分词处理，并根据停用词表删去了部分词。同时完成了一些文本错误的清除，分词结果中的非法空格的删除等工作。
	
### 6. 错误的处理：
无论是访问网页超时，下载附件失败，还是网页无标题无正文的情况，程序都做了对应的错误处理和错误提示输出，方便后续调试。
